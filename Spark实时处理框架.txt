基于Spark的流处理框架
项目背景：
    离线处理/批处理：慕课网的访问日志->点击、搜索
    实时处理：订单日志
        谁、时间、下单课程、支付、IP(运营商、地市)、UA

现有的流处理系统：
    Spark Streaming
    Structured Streaming(*****未来趋势)
    Flik
    Storm
    Kafka Stream

本项目的架构及处理流程：
    Log ==> Flume ==> Kafka ==> SparkStreaming(Direat) ==> Redis

项目需求：
    1)统计每天付费成功的总订单数、订单总金额
    2)付费订单占总下单的占比

Spark Streaming来进行统计分析，分析结果写入到Redis中(数据类型如何选择？？)
    最后选用hash类型进行存储，
    redis相关命令：
        KEYS * 查看有哪些key，
        TYPE key 查看key的类型
        HKEYS key 查看hash类型的key有哪些fields
        HGET key field 查看key下field的值

Spark Streaming&Kafka&Redis整合：
    a.生产者
        日志的格式(以json格式提供)：  time,userid,courseid,orderid,fee,flag
                   字段意思： 下单时间，用户id，课程id，订单id，费用，是否已付款
        这里用代码生成数据的方式，用Kafka生产者生产；后续可以生成文件，用flume读取数据，sink到Kafka的方式
    b.消费者
        SparkStreaming 读取kafka的数据，通过fastjson的方式把我们所需要的字段解析出来；
        然后根据不同的业务选择合适的redis的数据类型进行存储。
        Kafka offset问题：
            如果不管理Kafka的offset，那么SparkStreaming只能拿到最新的数据或者从头开始拿数据，这数据会有丢失或重复。
            正确的做法：第一次应用程序启动的时候，要从某个地方获取已经消费过的offset，然后当业务逻辑处理完之后，把处理完的offset保存到某个地方
            offset存储的地方：
            CheckPoint(不推荐)
            Kafka
            ZK/Mysql/HBase/Redis